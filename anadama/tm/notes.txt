Goals for this software:

Definitions:
Each set of data being processed by AnADAMA is called a 'flow'
A flow is composed of one (or more) runs numbers 1 thru n
  The number is based on how many times the flow was re-run due to errors or 
   new tool versions etc
Each run is composted of a number of tasks.
Each task contains input file(s), output file(s), and parent(s)
  who must complete prior to start

There should be a single json full DAG produced for each flow.
This DAG shouldn't change between runs.  Tasks that have (potentially)
completed should still be listed in the DAG.  The TM should
identify them as complete and update the GUI to reflect all
changes.

Each task should have a unique id associated with it (hex number?)

Each flow should be identified by a unique name:
  <Datatype> + <Date?> and or <unique ID>
ex: 
/usr/local/anadama_flows/16S_kbayer_1428
/usr/local/anadama_flows/16S_kbayer_1428_1
/usr/local/anadama_flows/16S_kbayer_1428_1/run1
/usr/local/anadama_flows/16S_kbayer_1428_1/run2
/usr/local/anadama_flows/16S_kbayer_1428_1/run3
/usr/local/anadama_flows/16S_kbayer_1428_1/run3/<tasknum><shortname>
/usr/local/anadama_flows/16S_kbayer_1428_1/run3/graph.html
/usr/local/anadama_flows/16S_kbayer_1428_1/run3/js/...

Current implementation is this:

current working directory is the default:
./anadama_flows/<hash>/run#

where <hash> is the first 5 characters of the md5 hash of the full
path to the first product in the list of task contained within the dag.
This hash should be unique and reproducible if the same data is run
multiple times.
 
Tornado changes:

In order to use tornado effectively, we need to modify the
current task management software:

1. use tornado process.subprocess for launching tasks.

2. no more looping in task mgr - should only wake up when
jobs finish?  What if jobs don't finish?  stuck jobs?
control through web gui?  Can we wake the task mgr 
periodically?

# background update every x seconds
    task = tornado.ioloop.PeriodicCallback(
            fx(x),
            15 * 1000)
(not currently needed - we wake up when a task completes
or fails)

websocket message passing:
1. graph (of dag) passed from server to client
message(dag):
return(graph)
2. individual task updates from server to client
return flowhash,
       run,
       task,
       status
3. update of task status in from current run
message(update)
return flowhash
       run
       list of waiting tasks,
       list of queued tasks,
       list of running tasks,
       list of finished tasks


client gets all updates (should strip out ones it doesn't
care about)

format?
assuming maps can be passed across a websocket:
1. map[graph] = { links: ... nodes: ... name: ... }
2. map[taskflow] = a38cd
   map[taskrun] = run2
   map[taskname] = ...
   map[taskstate] = [ WAITING|QUEUED|RUNNING|SUCCESSFUL COMPLETION|FAILED COMPLETION ]

--

In order to support multiple flows / runs happening simultaneously,
we need to split up the taskmanager into two parts.  The first
part would handle command line options and stdin json data
(just as the task manager does now).  However, this piece would
now only contain the cli.py file.  The parser.py, tm.py, and 
tasks.py, and tornado functionality would be housed separately 
in a daemon link tool.  So, the first piece would read in the
data, and would test to see if the daemon exists, if so, it
will communicate with it (via a websocket).  If the daemon 
doesn't exist, it will bring it up (and then communicate with
it).

The second piece (the daemon) will listen on a particular port
for connections sending json data for a particular flow and
run.  After which, it will emit events to the same websocket
for each change in event status.  The events will need to 
embed task flow and run to identify which flow the task 
status change corresponds to.  

Finally, a third message (current status request) needs to be
caught and processes by the server returning all of the task
status for the given flow/run.

--

List of websocket (json embedded) messages to the daemon:

1. new run json:
	mibc_build tasks json 
	anadama flow directory
	flow
	run
	tmp directory

2. status 
	status
	flow
	run

List of websocket (json embedded) messages from the daemon:

1. dag	
	graph

2. taskUpdate
	flow
	run
	task
